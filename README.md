# XGBoost

XGBoost is a supervised machine learning method for classification and regression and is used in the Train with AutoML tool. XGBoost is short for extreme gradient boosting. This method is based on decision trees and is an improvement over other methods, such as random forest and gradient boosting. It works well with large and complex datasets by using several optimization methods.

To fit a training dataset using XGBoost, an initial prediction is made. Residuals are calculated based on the predicted value and the observed values. A decision tree is created with the residuals using a similarity score of the residuals. The similarity of the data of a leaf is calculated, as well as the similarity gain of the subsequent split. The gains are compared to determine an entity and a threshold for a node. The output value for each leaf is also calculated using the residuals. For classification, the values are generally calculated using log odds and odds ratios. The output of the tree becomes the new residual for the dataset, which is used to build another tree. This process is repeated until the residuals stop decreasing, or the specified number of times. Each subsequent tree learns from the previous trees and is not assigned the same weight, unlike how Random Forest works.
